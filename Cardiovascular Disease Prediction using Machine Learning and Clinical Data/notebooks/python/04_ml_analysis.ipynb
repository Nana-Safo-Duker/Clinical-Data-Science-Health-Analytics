{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Analysis\n",
        "\n",
        "This notebook contains comprehensive machine learning analysis including:\n",
        "- Data Preprocessing\n",
        "- Feature Engineering\n",
        "- Model Training and Evaluation\n",
        "- Model Comparison\n",
        "- Hyperparameter Tuning\n",
        "- Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, classification_report, roc_curve, auc,\n",
        "                            roc_auc_score, precision_recall_curve)\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('../../data/Cardiovascular_Disease_Dataset.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "# Drop patient ID as it's not a feature\n",
        "X = df.drop(['target', 'patientid'], axis=1)\n",
        "y = df['target']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n",
        "print(f\"\\nTarget distribution percentage:\\n{y.value_counts(normalize=True) * 100}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values in features:\\n{X.isnull().sum()}\")\n",
        "print(f\"Missing values in target: {y.isnull().sum()}\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale numerical features\n",
        "numerical_cols = ['age', 'restingBP', 'serumcholestrol', 'maxheartrate', 'oldpeak']\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
        "print(f\"Scaled test set shape: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Use scaled data for models that need it\n",
        "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Cross-validation score\n",
        "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    else:\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    \n",
        "    # Classification Report\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
        "    'Precision': [r['precision'] for r in results.values()],\n",
        "    'Recall': [r['recall'] for r in results.values()],\n",
        "    'F1 Score': [r['f1_score'] for r in results.values()],\n",
        "    'ROC AUC': [r['roc_auc'] for r in results.values()],\n",
        "    'CV Accuracy': [r['cv_mean'] for r in results.values()],\n",
        "    'CV Std': [r['cv_std'] for r in results.values()]\n",
        "})\n",
        "\n",
        "results_df = results_df.sort_values('ROC AUC', ascending=False)\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0, 0].barh(results_df['Model'], results_df['Accuracy'], color='steelblue')\n",
        "axes[0, 0].set_xlabel('Accuracy')\n",
        "axes[0, 0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# ROC AUC comparison\n",
        "axes[0, 1].barh(results_df['Model'], results_df['ROC AUC'], color='coral')\n",
        "axes[0, 1].set_xlabel('ROC AUC')\n",
        "axes[0, 1].set_title('ROC AUC Comparison', fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# F1 Score comparison\n",
        "axes[1, 0].barh(results_df['Model'], results_df['F1 Score'], color='green')\n",
        "axes[1, 0].set_xlabel('F1 Score')\n",
        "axes[1, 0].set_title('F1 Score Comparison', fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Cross-validation accuracy\n",
        "axes[1, 1].barh(results_df['Model'], results_df['CV Accuracy'], xerr=results_df['CV Std'], color='purple')\n",
        "axes[1, 1].set_xlabel('Cross-Validation Accuracy')\n",
        "axes[1, 1].set_title('Cross-Validation Accuracy Comparison', fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curves for all models\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for name, result in results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
        "    roc_auc = result['roc_auc']\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves for All Models', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for top 3 models\n",
        "top_3_models = results_df.head(3)['Model'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, model_name in enumerate(top_3_models):\n",
        "    cm = confusion_matrix(y_test, results[model_name]['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
        "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[model_name][\"accuracy\"]:.3f}', \n",
        "                        fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].set_xticklabels(['No Disease', 'Disease'])\n",
        "    axes[idx].set_yticklabels(['No Disease', 'Disease'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
