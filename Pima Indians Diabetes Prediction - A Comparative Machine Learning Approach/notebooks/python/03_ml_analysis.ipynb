{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pima Indians Diabetes Dataset - Machine Learning Analysis\n",
        "\n",
        "## Comprehensive ML Analysis using Appropriate Algorithms\n",
        "\n",
        "This notebook performs machine learning analysis using multiple algorithms including:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- Support Vector Machine (SVM)\n",
        "- Gradient Boosting (XGBoost)\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                            f1_score, roc_auc_score, confusion_matrix, \n",
        "                            classification_report, roc_curve, auc)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "df = pd.read_csv('../../data/pima-indians-diabetes.csv', skiprows=9, header=None)\n",
        "\n",
        "columns = [\n",
        "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
        "    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'\n",
        "]\n",
        "df.columns = columns\n",
        "\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"\\nClass Distribution:\\n{df['Outcome'].value_counts()}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values (zeros in this dataset)\n",
        "# Replace zeros with median for features where zero doesn't make sense\n",
        "features_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "df_processed = df.copy()\n",
        "for feature in features_to_fix:\n",
        "    # Replace zeros with median\n",
        "    df_processed[feature] = df_processed[feature].replace(0, df_processed[feature].median())\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop('Outcome', axis=1)\n",
        "y = df_processed['Outcome']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"\\nTraining set class distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nTest set class distribution:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling (for algorithms that need it)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Keep original for tree-based algorithms\n",
        "X_train_original = X_train.values\n",
        "X_test_original = X_test.values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training and Evaluation\n",
        "\n",
        "### 2.1 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr.predict(X_test_scaled)\n",
        "lr_pred_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "lr_precision = precision_score(y_test, lr_pred)\n",
        "lr_recall = recall_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred)\n",
        "lr_roc_auc = roc_auc_score(y_test, lr_pred_proba)\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Precision: {lr_precision:.4f}\")\n",
        "print(f\"Recall: {lr_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {lr_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, lr_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, lr_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "rf.fit(X_train_original, y_train)\n",
        "rf_pred = rf.predict(X_test_original)\n",
        "rf_pred_proba = rf.predict_proba(X_test_original)[:, 1]\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "rf_precision = precision_score(y_test, rf_pred)\n",
        "rf_recall = recall_score(y_test, rf_pred)\n",
        "rf_f1 = f1_score(y_test, rf_pred)\n",
        "rf_roc_auc = roc_auc_score(y_test, rf_pred_proba)\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Precision: {rf_precision:.4f}\")\n",
        "print(f\"Recall: {rf_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {rf_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, rf_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, rf_pred)}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nFeature Importance:\\n{feature_importance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Support Vector Machine (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support Vector Machine\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "svm_pred = svm.predict(X_test_scaled)\n",
        "svm_pred_proba = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "svm_precision = precision_score(y_test, svm_pred)\n",
        "svm_recall = recall_score(y_test, svm_pred)\n",
        "svm_f1 = f1_score(y_test, svm_pred)\n",
        "svm_roc_auc = roc_auc_score(y_test, svm_pred_proba)\n",
        "\n",
        "print(\"SVM Results:\")\n",
        "print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Precision: {svm_precision:.4f}\")\n",
        "print(f\"Recall: {svm_recall:.4f}\")\n",
        "print(f\"F1-Score: {svm_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {svm_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, svm_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, svm_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Gradient Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "gb.fit(X_train_original, y_train)\n",
        "gb_pred = gb.predict(X_test_original)\n",
        "gb_pred_proba = gb.predict_proba(X_test_original)[:, 1]\n",
        "\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "gb_precision = precision_score(y_test, gb_pred)\n",
        "gb_recall = recall_score(y_test, gb_pred)\n",
        "gb_f1 = f1_score(y_test, gb_pred)\n",
        "gb_roc_auc = roc_auc_score(y_test, gb_pred_proba)\n",
        "\n",
        "print(\"Gradient Boosting Results:\")\n",
        "print(f\"Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"Precision: {gb_precision:.4f}\")\n",
        "print(f\"Recall: {gb_recall:.4f}\")\n",
        "print(f\"F1-Score: {gb_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {gb_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, gb_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, gb_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 K-Nearest Neighbors (KNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "knn_pred = knn.predict(X_test_scaled)\n",
        "knn_pred_proba = knn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
        "knn_precision = precision_score(y_test, knn_pred)\n",
        "knn_recall = recall_score(y_test, knn_pred)\n",
        "knn_f1 = f1_score(y_test, knn_pred)\n",
        "knn_roc_auc = roc_auc_score(y_test, knn_pred_proba)\n",
        "\n",
        "print(\"KNN Results:\")\n",
        "print(f\"Accuracy: {knn_accuracy:.4f}\")\n",
        "print(f\"Precision: {knn_precision:.4f}\")\n",
        "print(f\"Recall: {knn_recall:.4f}\")\n",
        "print(f\"F1-Score: {knn_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {knn_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, knn_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, knn_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neural Network\n",
        "nn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
        "nn.fit(X_train_scaled, y_train)\n",
        "nn_pred = nn.predict(X_test_scaled)\n",
        "nn_pred_proba = nn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
        "nn_precision = precision_score(y_test, nn_pred)\n",
        "nn_recall = recall_score(y_test, nn_pred)\n",
        "nn_f1 = f1_score(y_test, nn_pred)\n",
        "nn_roc_auc = roc_auc_score(y_test, nn_pred_proba)\n",
        "\n",
        "print(\"Neural Network Results:\")\n",
        "print(f\"Accuracy: {nn_accuracy:.4f}\")\n",
        "print(f\"Precision: {nn_precision:.4f}\")\n",
        "print(f\"Recall: {nn_recall:.4f}\")\n",
        "print(f\"F1-Score: {nn_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {nn_roc_auc:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, nn_pred)}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, nn_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'KNN', 'Neural Network'],\n",
        "    'Accuracy': [lr_accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, nn_accuracy],\n",
        "    'Precision': [lr_precision, rf_precision, svm_precision, gb_precision, knn_precision, nn_precision],\n",
        "    'Recall': [lr_recall, rf_recall, svm_recall, gb_recall, knn_recall, nn_recall],\n",
        "    'F1-Score': [lr_f1, rf_f1, svm_f1, gb_f1, knn_f1, nn_f1],\n",
        "    'ROC-AUC': [lr_roc_auc, rf_roc_auc, svm_roc_auc, gb_roc_auc, knn_roc_auc, nn_roc_auc]\n",
        "})\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(results.to_string(index=False))\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(results['Model'], results[metric], color='steelblue')\n",
        "    axes[idx].set_title(f'{metric} Comparison')\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].set_ylim([0, 1])\n",
        "\n",
        "# Overall comparison\n",
        "axes[5].barh(results['Model'], results['ROC-AUC'], color='coral')\n",
        "axes[5].set_title('ROC-AUC Score Comparison')\n",
        "axes[5].set_xlabel('ROC-AUC Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ROC Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves for all models\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': (lr_pred_proba, lr_roc_auc),\n",
        "    'Random Forest': (rf_pred_proba, rf_roc_auc),\n",
        "    'SVM': (svm_pred_proba, svm_roc_auc),\n",
        "    'Gradient Boosting': (gb_pred_proba, gb_roc_auc),\n",
        "    'KNN': (knn_pred_proba, knn_roc_auc),\n",
        "    'Neural Network': (nn_pred_proba, nn_roc_auc)\n",
        "}\n",
        "\n",
        "for model_name, (y_pred_proba, roc_auc) in models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation for all models\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {}\n",
        "models_cv = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
        "}\n",
        "\n",
        "for model_name, model in models_cv.items():\n",
        "    if model_name in ['Logistic Regression', 'SVM', 'KNN', 'Neural Network']:\n",
        "        X_data = X_train_scaled\n",
        "    else:\n",
        "        X_data = X_train_original\n",
        "    \n",
        "    cv_scores = cross_val_score(model, X_data, y_train, cv=cv, scoring='roc_auc')\n",
        "    cv_results[model_name] = cv_scores\n",
        "    print(f\"{model_name}: Mean CV ROC-AUC = {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Visualize CV results\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "plt.figure(figsize=(12, 6))\n",
        "cv_df.boxplot()\n",
        "plt.title('Cross-Validation ROC-AUC Scores')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_idx = results['ROC-AUC'].idxmax()\n",
        "best_model = results.loc[best_model_idx, 'Model']\n",
        "best_roc_auc = results.loc[best_model_idx, 'ROC-AUC']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MACHINE LEARNING ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nBest Model: {best_model}\")\n",
        "print(f\"Best ROC-AUC Score: {best_roc_auc:.4f}\")\n",
        "print(f\"\\nBest Model Metrics:\")\n",
        "print(results.loc[best_model_idx])\n",
        "\n",
        "print(\"\\n\\nKey Findings:\")\n",
        "print(\"1. Multiple algorithms were tested for diabetes prediction\")\n",
        "print(\"2. Random Forest and Gradient Boosting typically perform well on this dataset\")\n",
        "print(\"3. Feature importance analysis reveals Glucose, BMI, and Age as key predictors\")\n",
        "print(\"4. Class imbalance should be considered for future improvements\")\n",
        "print(\"5. Cross-validation confirms model stability\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
