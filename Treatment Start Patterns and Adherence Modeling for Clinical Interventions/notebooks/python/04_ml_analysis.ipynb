{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Analysis\n",
        "\n",
        "This notebook applies machine learning algorithms to analyze the treatment starts dataset.\n",
        "\n",
        "## Objectives\n",
        "- Feature engineering\n",
        "- Model selection and training\n",
        "- Model evaluation\n",
        "- Prediction and insights\n",
        "- Comparison of different algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv('../../data/mock_treatment_starts_2016.csv')\n",
        "df['TreatmentStart'] = pd.to_datetime(df['TreatmentStart'], format='%m/%d/%y')\n",
        "df['Year'] = df['TreatmentStart'].dt.year\n",
        "df['Month'] = df['TreatmentStart'].dt.month\n",
        "df['Day'] = df['TreatmentStart'].dt.day\n",
        "df['Weekday'] = df['TreatmentStart'].dt.dayofweek\n",
        "df['Quarter'] = df['TreatmentStart'].dt.quarter\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering\n",
        "# Handle outlier (PT12 with dosage 1800 - likely data entry error, replace with median)\n",
        "df_ml = df.copy()\n",
        "outlier_idx = df_ml[df_ml['Dosage'] > 1000].index\n",
        "if len(outlier_idx) > 0:\n",
        "    df_ml.loc[outlier_idx, 'Dosage'] = df_ml['Dosage'].median()\n",
        "    print(f\"Outliers handled: {len(outlier_idx)} records\")\n",
        "\n",
        "# Encode categorical variables\n",
        "le_drug = LabelEncoder()\n",
        "df_ml['Drug_encoded'] = le_drug.fit_transform(df_ml['Drug'])\n",
        "\n",
        "# Create features\n",
        "X = df_ml[['Month', 'Day', 'Weekday', 'Quarter', 'Drug_encoded']].copy()\n",
        "y = df_ml['Dosage'].copy()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "print(f\"Target: Dosage\")\n",
        "print(f\"\\nFeature shapes: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature statistics:\")\n",
        "print(X.describe())\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(y.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Splitting and Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (for algorithms that require scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA SPLITTING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Train target mean: {y_train.mean():.2f}\")\n",
        "print(f\"Test target mean: {y_test.mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Lasso Regression': Lasso(alpha=1.0),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=5),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=3),\n",
        "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=3)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL TRAINING AND EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use scaled data for linear models\n",
        "    if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression']:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    results[name] = {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  RMSE: {rmse:.2f}\")\n",
        "    print(f\"  MAE: {mae:.2f}\")\n",
        "    print(f\"  R²: {r2:.4f}\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. RMSE comparison\n",
        "results_df_sorted = results_df.sort_values('RMSE')\n",
        "axes[0, 0].barh(results_df_sorted.index, results_df_sorted['RMSE'], color='steelblue')\n",
        "axes[0, 0].set_title('Model RMSE Comparison')\n",
        "axes[0, 0].set_xlabel('RMSE')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 2. R² comparison\n",
        "results_df_sorted_r2 = results_df.sort_values('R2', ascending=False)\n",
        "axes[0, 1].barh(results_df_sorted_r2.index, results_df_sorted_r2['R2'], color='coral')\n",
        "axes[0, 1].set_title('Model R² Comparison')\n",
        "axes[0, 1].set_xlabel('R² Score')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 3. Best model predictions vs actual\n",
        "best_model_name = results_df.idxmin()['RMSE']\n",
        "if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression']:\n",
        "    best_model = models[best_model_name]\n",
        "    best_model.fit(X_train_scaled, y_train)\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "else:\n",
        "    best_model = models[best_model_name]\n",
        "    best_model.fit(X_train, y_train)\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "axes[1, 0].scatter(y_test, y_pred_best, alpha=0.6, color='teal')\n",
        "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1, 0].set_xlabel('Actual Dosage')\n",
        "axes[1, 0].set_ylabel('Predicted Dosage')\n",
        "axes[1, 0].set_title(f'Predictions vs Actual ({best_model_name})')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Residuals plot\n",
        "residuals = y_test - y_pred_best\n",
        "axes[1, 1].scatter(y_pred_best, residuals, alpha=0.6, color='purple')\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1, 1].set_xlabel('Predicted Dosage')\n",
        "axes[1, 1].set_ylabel('Residuals')\n",
        "axes[1, 1].set_title(f'Residuals Plot ({best_model_name})')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"  RMSE: {results_df.loc[best_model_name, 'RMSE']:.2f}\")\n",
        "print(f\"  R²: {results_df.loc[best_model_name, 'R2']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (for tree-based models)\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGBoost']\n",
        "for idx, model_name in enumerate(tree_models):\n",
        "    if model_name in models:\n",
        "        model = models[model_name]\n",
        "        if model_name == 'XGBoost':\n",
        "            model.fit(X_train, y_train)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "        \n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            \n",
        "            row = idx // 2\n",
        "            col = idx % 2\n",
        "            axes[row, col].barh(importance['feature'], importance['importance'], color='steelblue')\n",
        "            axes[row, col].set_title(f'Feature Importance: {model_name}')\n",
        "            axes[row, col].set_xlabel('Importance')\n",
        "            axes[row, col].grid(True, alpha=0.3, axis='x')\n",
        "            \n",
        "            print(f\"\\n{model_name}:\")\n",
        "            print(importance)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation for best models\n",
        "print(\"=\" * 60)\n",
        "print(\"CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cv_results = {}\n",
        "top_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Decision Tree']\n",
        "\n",
        "for model_name in top_models:\n",
        "    if model_name in models:\n",
        "        model = models[model_name]\n",
        "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "        cv_rmse = np.sqrt(-cv_scores)\n",
        "        cv_results[model_name] = {\n",
        "            'Mean RMSE': cv_rmse.mean(),\n",
        "            'Std RMSE': cv_rmse.std(),\n",
        "            'Scores': cv_rmse\n",
        "        }\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Mean RMSE: {cv_rmse.mean():.2f} (+/- {cv_rmse.std() * 2:.2f})\")\n",
        "\n",
        "# Visualize CV results\n",
        "cv_df = pd.DataFrame({k: v['Scores'] for k, v in cv_results.items()})\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "cv_df.boxplot(ax=ax)\n",
        "ax.set_title('Cross-Validation RMSE Distribution')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
