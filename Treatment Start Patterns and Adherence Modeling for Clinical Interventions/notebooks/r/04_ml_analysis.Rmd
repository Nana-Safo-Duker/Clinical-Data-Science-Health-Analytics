---
title: "Machine Learning Analysis"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Objectives
- Feature engineering
- Model selection and training
- Model evaluation
- Prediction and insights

```{r load-libraries}
library(dplyr)
library(caret)
library(randomForest)
library(gbm)
library(xgboost)
library(ggplot2)
library(gridExtra)
library(lubridate)
```

```{r load-data}
df <- read.csv("../../data/mock_treatment_starts_2016.csv", stringsAsFactors = FALSE)
df$TreatmentStart <- as.Date(df$TreatmentStart, format = "%m/%d/%y")
df$Year <- year(df$TreatmentStart)
df$Month <- month(df$TreatmentStart)
df$Day <- day(df$TreatmentStart)
df$Weekday <- wday(df$TreatmentStart)
df$Quarter <- quarter(df$TreatmentStart)

# Handle outlier
df_ml <- df
outlier_idx <- which(df_ml$Dosage > 1000)
if (length(outlier_idx) > 0) {
  df_ml$Dosage[outlier_idx] <- median(df_ml$Dosage)
  cat("Outliers handled:", length(outlier_idx), "records\n")
}

# Encode categorical variables
df_ml$Drug_encoded <- ifelse(df_ml$Drug == "Cisplatin", 0, 1)

# Create features
X <- df_ml[, c("Month", "Day", "Weekday", "Quarter", "Drug_encoded")]
y <- df_ml$Dosage

cat("Dataset loaded successfully!\n")
cat("Shape:", dim(df_ml), "\n")
head(df_ml)
```

## 1. Feature Engineering

```{r feature-engineering}
cat("=" %+% rep("=", 59), "\n")
cat("FEATURE ENGINEERING\n")
cat("=" %+% rep("=", 59), "\n")

cat("\nFeatures:", paste(colnames(X), collapse = ", "), "\n")
cat("Target: Dosage\n")
cat("\nFeature statistics:\n")
summary(X)
cat("\nTarget statistics:\n")
summary(y)
```

## 2. Data Splitting

```{r data-splitting}
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

cat("=" %+% rep("=", 59), "\n")
cat("DATA SPLITTING\n")
cat("=" %+% rep("=", 59), "\n")
cat("Training set:", nrow(X_train), "samples\n")
cat("Test set:", nrow(X_test), "samples\n")
cat("Train target mean:", mean(y_train), "\n")
cat("Test target mean:", mean(y_test), "\n")
```

## 3. Model Training and Evaluation

```{r model-training}
cat("=" %+% rep("=", 59), "\n")
cat("MODEL TRAINING AND EVALUATION\n")
cat("=" %+% rep("=", 59), "\n")

# Linear Regression
lm_model <- lm(y_train ~ ., data = data.frame(X_train, y_train = y_train))
lm_pred <- predict(lm_model, newdata = data.frame(X_test))
lm_rmse <- sqrt(mean((y_test - lm_pred)^2))
lm_r2 <- cor(y_test, lm_pred)^2

cat("\nLinear Regression:\n")
cat("  RMSE:", lm_rmse, "\n")
cat("  R²:", lm_r2, "\n")

# Random Forest
rf_model <- randomForest(x = X_train, y = y_train, ntree = 100, mtry = 2)
rf_pred <- predict(rf_model, newdata = X_test)
rf_rmse <- sqrt(mean((y_test - rf_pred)^2))
rf_r2 <- cor(y_test, rf_pred)^2

cat("\nRandom Forest:\n")
cat("  RMSE:", rf_rmse, "\n")
cat("  R²:", rf_r2, "\n")

# Gradient Boosting
gbm_model <- gbm(y_train ~ ., data = data.frame(X_train, y_train = y_train),
                 n.trees = 100, interaction.depth = 3, distribution = "gaussian")
gbm_pred <- predict(gbm_model, newdata = data.frame(X_test), n.trees = 100)
gbm_rmse <- sqrt(mean((y_test - gbm_pred)^2))
gbm_r2 <- cor(y_test, gbm_pred)^2

cat("\nGradient Boosting:\n")
cat("  RMSE:", gbm_rmse, "\n")
cat("  R²:", gbm_r2, "\n")

# Results comparison
results <- data.frame(
  Model = c("Linear Regression", "Random Forest", "Gradient Boosting"),
  RMSE = c(lm_rmse, rf_rmse, gbm_rmse),
  R2 = c(lm_r2, rf_r2, gbm_r2)
)
cat("\nModel Comparison:\n")
print(results)
```

## 4. Model Visualization

```{r model-visualization}
# Find best model
best_model_name <- results$Model[which.min(results$RMSE)]
if (best_model_name == "Linear Regression") {
  best_pred <- lm_pred
} else if (best_model_name == "Random Forest") {
  best_pred <- rf_pred
} else {
  best_pred <- gbm_pred
}

# Predictions vs Actual
p1 <- ggplot(data.frame(Actual = y_test, Predicted = best_pred), 
             aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "teal") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = paste("Predictions vs Actual (", best_model_name, ")"),
       x = "Actual Dosage", y = "Predicted Dosage") +
  theme_minimal()

# Residuals plot
residuals <- y_test - best_pred
p2 <- ggplot(data.frame(Predicted = best_pred, Residuals = residuals),
             aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = paste("Residuals Plot (", best_model_name, ")"),
       x = "Predicted Dosage", y = "Residuals") +
  theme_minimal()

grid.arrange(p1, p2, nrow = 1, ncol = 2)

cat("\nBest Model:", best_model_name, "\n")
cat("  RMSE:", min(results$RMSE), "\n")
cat("  R²:", results$R2[which.min(results$RMSE)], "\n")
```

## 5. Feature Importance

```{r feature-importance}
# Feature importance for Random Forest
if (exists("rf_model")) {
  importance_df <- data.frame(
    Feature = rownames(rf_model$importance),
    Importance = rf_model$importance[, 1]
  )
  importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
  
  p <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(title = "Feature Importance: Random Forest",
         x = "Feature", y = "Importance") +
    theme_minimal()
  print(p)
  
  cat("\nFeature Importance (Random Forest):\n")
  print(importance_df)
}
```

```{r helper}
`%+%` <- function(a, b) paste0(a, b)
```


